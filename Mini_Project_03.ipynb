{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Necessary Library Imports"
      ],
      "metadata": {
        "id": "lijMAC8dVHCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "D9rHBNX-yxKM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the Data"
      ],
      "metadata": {
        "id": "T4pj1oveVUKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount the Google Drive"
      ],
      "metadata": {
        "id": "1aUChWxUVViu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "zVVxY3ujzFZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609b7fe1-c1ad-4051-ddac-4ea406cde67d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read the data file"
      ],
      "metadata": {
        "id": "NN3qvtuGVifP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/My Drive/Colab_Data_Files/sin.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "id": "ETwXiUN6zHsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a2c20c-354c-45df-9d70-e5c4344c0a23"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you will receive a package in the mail\tඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\n",
            "you will receive a confirmation code after completing the registration\tලියාපදිංචිය සම්පූර්ණ කලාට පස්සෙ ඔයාට තහවුරු කිරීමේ කේතයක්  හම්බවේවි\n",
            "you will receive a discount on your next purchase\tඊලඟ මිලදී ගැනීම කරන කොට ඔයාට වට්ටමක් හම්බවේවි\n",
            "you will receive a phone call from our customer service team\tඔයාට අපේ පාරිභෝගික සේවා කණ්ඩායමෙන් දුරකථන ඇමතුමක් හම්බවේවි\n",
            "you will receive a notification when your order is ready for pickup\tඔයාගේ ඇණවුම සූදානම් උනාට පස්සෙ ඔයාට දැනුම් දීමක් හම්බවේවි\n",
            "you will receive a response to your inquiry within 24 hours\tඔයාට පැය 24ක් ඇතුලත ඔයාගෙ විමසීමට පිළිතුරක්  හම්බවේවි\n",
            "you will receive a gift for your loyalty\tඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\n",
            "you will receive an invitation to the event\tඔයාට උත්සවයට ආරාධනාවක් හම්බවේවි\n",
            "you will receive a refund for the returned item\tඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\n",
            "you'll receive an email with the details\tඔයාට විස්තර එක්ක  ඊමේල් එකක් හම්බවේවි\n",
            "you'll receive a package in the mail\tඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\n",
            "you'll receive a refund for the returned item\tඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\n",
            "you'll receive a gift for your loyalty\tඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\n",
            "your mother will receive a car\tඔයාගෙ අම්මට කාර් එකක් හම්බවේවි\n",
            "your brother will receive a car\tඔයාගෙ සහෝදරයට කාර් එකක් හම්බවේවි\n",
            "it's pretty cool\tඒක මාර ලස්සනයි\n",
            "we're inviting you to do that\tඅපි ඔයාට ආරාධනා කරනවා ඒක කරන්න කියලා\n",
            "join us in improving machine translation\tයන්ත්‍ර පරිවර්තනය දියුණු කරන්න අපිත් එක්ක එකතු වෙන්න\n",
            "your translations will be used to train an nmt model\tඔයාගෙ පරිවර්තන NMT මොඩලය පුහුණු කරන්න පාවිච්චි කරාවි\n",
            "we're inviting you to submit translation samples for the zoomnmt training\tඅපි ඔයාට ආරාධනා කරනවා  ZoomNMT පුහුණු කරන්න පරිවර්තන සාම්පල ඉදිරිපත් කරන්න කියලා\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHvEa77mECHo",
        "outputId": "ae69e7e9-02d6-42d8-b093-6993877f957f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a small village there lived a young girl named Lily who had a special gift for growing the most beautiful flowers\tඑක් කලෙක කුඩා ගමක ලිලී නම් තරුණියක් ජීවත් වූ අතර ඇයට ලස්සනම මල් වගා කිරීම සඳහා විශේෂ තෑග්ගක් තිබුණි\n",
            "In a distant land a brave knight named Sir Arthur embarked on a perilous journey to rescue a kidnapped princess from an evil sorcerer\tදුර රටක ශ්‍රීමත් ආතර් නම් නිර්භීත නයිට්වරයා නපුරු මායාකාරයෙකුගෙන් පැහැරගෙන ගිය කුමරියක් බේරා ගැනීමට භයානක ගමනක් ආරම්භ කළේය\n",
            "Emily a curious explorer set out on an adventure to uncover the hidden treasures of an ancient lost city deep in the Amazon rainforest\tකුතුහලය දනවන ගවේෂකයෙකු වන එමිලි ඇමසන් වනාන්තරයේ ගැඹුරින් ගිලිහී ගිය පැරණි නගරයක සැඟවුණු වස්තු සොයා ගැනීමට ත්‍රාසජනක ගමනක් ආරම්භ කළාය\n",
            "In the peaceful town of Willowbrook a mischievous cat named Oliver had a talent for getting into amusing and unexpected predicaments\tසාමකාමී නගරයක් වන විලෝබෲක්හි ඔලිවර් නම් දඟකාර බළලාට විනෝදජනක සහ අනපේක්ෂිත දුෂ්කරතාවන්ට පත්වීමේ දක්ෂතාවයක් තිබුණි\n",
            "Sarah a talented pianist dreamt of performing on the grand stage of Carnegie Hall and spent countless hours practicing her musical craft\tදක්ෂ පියානෝ වාදකයෙකු වන සාරා Carnegie Hall හි මහා වේදිකාවේ රඟ දැක්වීමට සිහින මැවූ අතර ඇගේ සංගීත ශිල්පය පුහුණු කිරීමට පැය ගණන් ගත කළාය\n",
            "Deep in the enchanted forest a group of woodland creatures led by a wise old owl embarked on a quest to save their home from destruction\tවශීකෘත වනාන්තරයේ ගැඹුරින් නුවණැති මහලු බකමූණෙකු විසින් මෙහෙයවන ලද වනාන්තර ජීවීන් කණ්ඩායමක් තම නිවස විනාශයෙන් බේරා ගැනීමේ ගවේෂණයක යෙදී සිටියහ\n",
            "On a sunny summer day a group of friends gathered at the beach for a fun-filled day of sandcastle building swimming and laughter\tඅව්ව සහිත ගිම්හාන දිනයක මිතුරන් පිරිසක් වැලි මාලිගා ගොඩ නැගීම පිහිනීම සහ සිනහවෙන් විනෝදයෙන් පිරුණු දවසක් සඳහා වෙරළට රැස් වූහ\n",
            "In a quaint seaside village a mysterious stranger arrived bringing with them an air of excitement and an unexpected twist to the townspeople's lives\tවිචිත්‍රවත් මුහුදු වෙරළේ ගම්මානයකට අද්භූත ආගන්තුකයෙකු පැමිණියේ ඔවුන් සමඟ උද්දීපනයක් සහ නගර වැසියන්ගේ ජීවිතයට අනපේක්ෂිත පෙරළියක් ගෙන එයි\n",
            "Thomas an aspiring writer found inspiration in the bustling streets of a vibrant city where every corner held a story waiting to be told\tඅභිලාෂකාමී ලේඛකයෙකු වන තෝමස් සෑම අස්සක් මුල්ලක් නෑරම කීමට බලා සිටින කතන්දර ඇති උද්යෝගිමත් නගරයක කාර්යබහුල වීදිවල ආශ්වාදයක් ලබා ගත්තේය\n",
            "In a land of mythical creatures a young dragon named Ember struggled to control her fiery breath while learning valuable lessons of friendship and courage\tමිථ්‍යා ජීවීන් සිටින රටක එම්බර් නම් තරුණ මකරෙක් මිත්‍රත්වයේ සහ ධෛර්‍යයේ වටිනා පාඩම් ඉගෙන ගනිමින් ඇගේ ගිනි හුස්ම පාලනය කිරීමට මහත් පරිශ්‍රමයක් දැරීය\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the English and Sinhala translation pairs"
      ],
      "metadata": {
        "id": "QzZUua-VV8Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, sinhala = line.split(\"\\t\")\n",
        "    sinhala = \"[start] \" + sinhala + \" [end]\"\n",
        "    text_pairs.append((english, sinhala))\n",
        "for i in range(3):\n",
        "  print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "w582iaWz8hlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcec9b13-aafe-4f22-a3ed-f372f5ac4de4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"i'll let you pack\", '[start] මම ඔයාට බඩු ලෑස්ති කරගන්න දෙන්නම් [end]')\n",
            "(\"i'm not like that\", '[start] මම එහෙම නෑ [end]')\n",
            "('i have a great idea to talk as much as we can', '[start] මට පොපියන අදහසක් තියෙනවා අපිට පුළුවන් ගානකට කතා කරගන්න [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Randomize the data\n"
      ],
      "metadata": {
        "id": "5EhlPDehWL-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "WonoPO4oEsVW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spliting the data into training, validation and Testing"
      ],
      "metadata": {
        "id": "IhAcd0q8WP0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoKl4CnQEyfx",
        "outputId": "b06431b3-ae25-457f-82a8-da266d98b91d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 80684\n",
            "Training set size: 56480\n",
            "Validation set size: 12102\n",
            "Testing set size: 12102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_InNACfgIypi",
        "outputId": "bb6a7bb7-d364-4fc6-e924-2822293704ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80684"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing Punctuations"
      ],
      "metadata": {
        "id": "yN54-EHCWT2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")"
      ],
      "metadata": {
        "id": "i3SM4fSJI1j8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f\"[{re.escape(strip_chars)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5oJtiTLnI4FD",
        "outputId": "04cffda9-0c0c-4167-f703-22fb2e6076b7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorizing the English and Sinhala text pairs\n"
      ],
      "metadata": {
        "id": "-KeJJX6ZWWnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ],
      "metadata": {
        "id": "h5SdNqrXFdaH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing datasets for the translation task"
      ],
      "metadata": {
        "id": "Ix94L0ncWf97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "def format_dataset(eng, sin):\n",
        "    eng = source_vectorization(eng)\n",
        "    sin = target_vectorization(sin)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"sinhala\": sin[:, :-1],\n",
        "    }, sin[:, 1:])\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, sin_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    sin_texts = list(sin_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "inputs['english'].shape: (64, 20)\n",
        "inputs['sinhala'].shape: (64, 20)\n",
        "targets.shape: (64, 20)\n",
        "print(list(train_ds.as_numpy_iterator())[50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_ggqgh8Fqh4",
        "outputId": "7d63b125-e0ba-46fa-881f-feecc2141ae2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n",
            "({'english': array([[  32,   20,  674, ...,    0,    0,    0],\n",
            "       [ 772,    4,  570, ...,    0,    0,    0],\n",
            "       [1157, 2087,    0, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [1923, 1923, 1078, ...,    0,    0,    0],\n",
            "       [   4,  305, 1350, ...,    0,    0,    0],\n",
            "       [ 286,   49,  251, ...,    0,    0,    0]]), 'sinhala': array([[   2,   12,  131, ...,    0,    0,    0],\n",
            "       [   2, 1152, 2503, ...,    0,    0,    0],\n",
            "       [   2, 1392, 2758, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2, 2744, 2744, ...,    0,    0,    0],\n",
            "       [   2, 1823, 2643, ...,    0,    0,    0],\n",
            "       [   2,   56,   10, ...,    0,    0,    0]])}, array([[  12,  131,  102, ...,    0,    0,    0],\n",
            "       [1152, 2503,   14, ...,    0,    0,    0],\n",
            "       [1392, 2758,    3, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [2744, 2744, 5554, ...,    0,    0,    0],\n",
            "       [1823, 2643,   18, ...,    0,    0,    0],\n",
            "       [  56,   10, 3501, ...,    0,    0,    0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer encoder implemented as a subclassed Layer"
      ],
      "metadata": {
        "id": "OV_aROU_Wmc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return confi"
      ],
      "metadata": {
        "id": "Eg5Yf_i0GYz4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Transformer decoder"
      ],
      "metadata": {
        "id": "HM3kvFSFW7Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ],
      "metadata": {
        "id": "EoxPZhOBGlMn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Encoding"
      ],
      "metadata": {
        "id": "yaB15lHvXAvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "M-wxP9CEGs6a"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#End-to-end Transformer"
      ],
      "metadata": {
        "id": "EtuAOyR4XLBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "k1HOQnqSG2Kx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWh95cCyHFrN",
        "outputId": "a8b2906e-2250-4cb9-87af-c108029ec40c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the sequence-to-sequence Transformer"
      ],
      "metadata": {
        "id": "SQKxOML0XSc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99dLZnnIHUCP",
        "outputId": "edc39411-2461-4cbb-f2e2-ff334df662d2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "883/883 [==============================] - 83s 84ms/step - loss: 4.6845 - accuracy: 0.4024 - val_loss: 3.9567 - val_accuracy: 0.4529\n",
            "Epoch 2/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 3.9171 - accuracy: 0.4669 - val_loss: 3.5533 - val_accuracy: 0.4942\n",
            "Epoch 3/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 3.6129 - accuracy: 0.4956 - val_loss: 3.3952 - val_accuracy: 0.5096\n",
            "Epoch 4/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.4242 - accuracy: 0.5167 - val_loss: 3.3323 - val_accuracy: 0.5198\n",
            "Epoch 5/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.2941 - accuracy: 0.5325 - val_loss: 3.3412 - val_accuracy: 0.5187\n",
            "Epoch 6/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.1902 - accuracy: 0.5467 - val_loss: 3.3482 - val_accuracy: 0.5220\n",
            "Epoch 7/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.1090 - accuracy: 0.5590 - val_loss: 3.3075 - val_accuracy: 0.5302\n",
            "Epoch 8/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.0420 - accuracy: 0.5699 - val_loss: 3.3888 - val_accuracy: 0.5256\n",
            "Epoch 9/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.9853 - accuracy: 0.5799 - val_loss: 3.3967 - val_accuracy: 0.5294\n",
            "Epoch 10/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.9308 - accuracy: 0.5894 - val_loss: 3.3631 - val_accuracy: 0.5348\n",
            "Epoch 11/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.8847 - accuracy: 0.5969 - val_loss: 3.3961 - val_accuracy: 0.5323\n",
            "Epoch 12/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.8449 - accuracy: 0.6040 - val_loss: 3.4415 - val_accuracy: 0.5335\n",
            "Epoch 13/30\n",
            "883/883 [==============================] - 64s 73ms/step - loss: 2.8048 - accuracy: 0.6109 - val_loss: 3.4312 - val_accuracy: 0.5336\n",
            "Epoch 14/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.7695 - accuracy: 0.6177 - val_loss: 3.4788 - val_accuracy: 0.5341\n",
            "Epoch 15/30\n",
            "883/883 [==============================] - 60s 69ms/step - loss: 2.7369 - accuracy: 0.6225 - val_loss: 3.4741 - val_accuracy: 0.5387\n",
            "Epoch 16/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.7109 - accuracy: 0.6265 - val_loss: 3.5006 - val_accuracy: 0.5265\n",
            "Epoch 17/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.6813 - accuracy: 0.6321 - val_loss: 3.5173 - val_accuracy: 0.5327\n",
            "Epoch 18/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.6626 - accuracy: 0.6357 - val_loss: 3.5418 - val_accuracy: 0.5299\n",
            "Epoch 19/30\n",
            "883/883 [==============================] - 65s 73ms/step - loss: 2.6355 - accuracy: 0.6403 - val_loss: 3.5807 - val_accuracy: 0.5327\n",
            "Epoch 20/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.6123 - accuracy: 0.6445 - val_loss: 3.6013 - val_accuracy: 0.5274\n",
            "Epoch 21/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.5887 - accuracy: 0.6482 - val_loss: 3.5597 - val_accuracy: 0.5309\n",
            "Epoch 22/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.5702 - accuracy: 0.6515 - val_loss: 3.6425 - val_accuracy: 0.5205\n",
            "Epoch 23/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.5467 - accuracy: 0.6552 - val_loss: 3.6167 - val_accuracy: 0.5306\n",
            "Epoch 24/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.5296 - accuracy: 0.6584 - val_loss: 3.6672 - val_accuracy: 0.5304\n",
            "Epoch 25/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.5097 - accuracy: 0.6620 - val_loss: 3.6474 - val_accuracy: 0.5235\n",
            "Epoch 26/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.4881 - accuracy: 0.6655 - val_loss: 3.7150 - val_accuracy: 0.5283\n",
            "Epoch 27/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.4690 - accuracy: 0.6685 - val_loss: 3.7273 - val_accuracy: 0.5246\n",
            "Epoch 28/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.4517 - accuracy: 0.6711 - val_loss: 3.7061 - val_accuracy: 0.5324\n",
            "Epoch 29/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.4354 - accuracy: 0.6743 - val_loss: 3.7248 - val_accuracy: 0.5278\n",
            "Epoch 30/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.4130 - accuracy: 0.6783 - val_loss: 3.7569 - val_accuracy: 0.5271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bd6a0412f80>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the vocabulary and index lookup for sinhala\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g10M5lXiRL9_",
        "outputId": "e7caf33e-b44d-4989-f77e-7a0e5a960b46"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "not your lucky day\n",
            "[start] ඔයාගෙ මෝඩ නෑ දවස [end]\n",
            "-\n",
            "the clink   did you make it  \n",
            "[start] මං ඔයාට ඒක කලා කියලා තේරුනාද [end]\n",
            "-\n",
            "stop this destruction\n",
            "[start] නවත්තන්න මේ [UNK] [end]\n",
            "-\n",
            "hmm\n",
            "[start] හ්ම්ම් [end]\n",
            "-\n",
            "give up\n",
            "[start] අතහරින්න [end]\n",
            "-\n",
            "he it's complicated\n",
            "[start] එයා ඒක අවුල් [end]\n",
            "-\n",
            "melanie let's go\n",
            "[start] [UNK] අපි යමු [end]\n",
            "-\n",
            " can you get up    hmmm\n",
            "[start] ඔයාට පුළුවන්ද හ්ම්ම් හ්ම්ම් [end]\n",
            "-\n",
            "work them to death\n",
            "[start] එයාලාට [UNK] [end]\n",
            "-\n",
            "i'm just a bystander\n",
            "[start] මම නිකම් [UNK] විතරයි [end]\n",
            "-\n",
            "you will understand when you see it\n",
            "[start] ඔයාට ඒක තේරුම් [UNK] නෑ නේද [end]\n",
            "-\n",
            " he wants a picture you tell the truth \n",
            "[start] එයාට ඕනේ ඔයා ඇත්ත කියලා අරගන්න [end]\n",
            "-\n",
            "gaburin is talking son\n",
            "[start] [UNK] කතා කරනවා පුතා [end]\n",
            "-\n",
            "look i'm not sure\n",
            "[start] බලන්න මට විශ්වාස නෑ [end]\n",
            "-\n",
            "simon kiely's death was an accident\n",
            "[start] සයිමන් [UNK] පිළිගන්නවා එකක් කියලා [end]\n",
            "-\n",
            "what do you want\n",
            "[start] මොනාද ඕනි [end]\n",
            "-\n",
            "black magic\n",
            "[start] බ්ලැක් මැජික් [end]\n",
            "-\n",
            "more  \n",
            "[start] තව [end]\n",
            "-\n",
            "be lively the channel wants you\n",
            "[start] ඔයා වෙන්නේ [UNK] ඕනේ නෑ [end]\n",
            "-\n",
            " yes i am single but with you i will not make friends for life\n",
            "[start] ඔව් මම ඔයා එක්ක [UNK] [UNK] [UNK] නෑ [end]\n"
          ]
        }
      ]
    }
  ]
}